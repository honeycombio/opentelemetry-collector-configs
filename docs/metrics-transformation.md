# Metrics Transformation for Wider Events in Honeycomb

Honeycomb accepts OTLP metrics and converts them into events, [as described in our documentation](https://docs.honeycomb.io/manage-data-volume/metrics/). To manage costs and improve usability of this data in Honeycomb, we suggest that Honeycomb metrics customers should pack as many metrics datapoints as possible into as few events as possible.

Unfortunately, many common metrics producers create metric data where almost every data point has a unique set of descriptive attributes. This will result in a repackaging ratio of close to 1:1, or one event per data point. Notably, this includes [OpenTelemetry Collector’s `hostmetrics` receiver](https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/receiver/hostmetricsreceiver#readme), which [we encourage you to use](https://docs.honeycomb.io/getting-data-in/metrics/opentelemetry-collector-host-metrics/) since it is the easiest way to obtain standard infrastructure-level metrics. Here’s an example of part of a metrics request generated by the hostmetrics receiver (or, [see the full request here](https://gist.github.com/maxedmands/ffaa8aff0bacab742fd0d94422646803)):

```
resource:{
    attributes:{ key:"host.name" value:{ string_value:"vm2" } }
    attributes:{ key:"service.name" value:{ string_value:"webserver" } }
}
metrics:{
  name:"system.memory.usage"
  description:"Bytes of memory in use."
  unit:"By"
  int_sum:{
    data_points:{
      labels:{ key:"state" value:"used" }
      time_unix_nano:1623277287515079370
      value:217608192
    }
    data_points:{
      labels:{ key:"state" value:"free" }
      time_unix_nano:1623277287515079370
      value:79982592
    }
    data_points:{
      labels:{ key:"state" value:"buffered" }
      time_unix_nano:1623277287515079370
      value:28291072
    }
    data_points:{
      labels:{ key:"state" value:"cached" }
      time_unix_nano:1623277287515079370
      value:703111168
    }
    data_points:{
      labels:{ key:"state" value:"slab_reclaimable" }
      time_unix_nano:1623277287515079370
      value:41844736
    }
    data_points:{
      labels:{ key:"state" value:"slab_unreclaimable" }
      time_unix_nano:1623277287515079370
      value:41148416
    }
    aggregation_temporality:AGGREGATION_TEMPORALITY_CUMULATIVE
  }
}
```

Here are the resulting events that are generated from this data. Note that we have received 6 data points, and we have generated 6 events, for a repackaging ratio of 1:

```json
{
  "Timestamp": "2021-06-09T22:21:27.51507937Z",
  "host.name": "vm2",
  "service.name": "webserver",
  "state": "used",
  "system.memory.usage": 217608192
},
{
  "Timestamp": "2021-06-09T22:21:27.51507937Z",
  "host.name": "vm2",
  "service.name": "webserver",
  "state": "slab_unreclaimable",
  "system.memory.usage": 41148416
},
{
  "Timestamp": "2021-06-09T22:21:27.51507937Z",
  "host.name": "vm2",
  "service.name": "webserver",
  "state": "cached",
  "system.memory.usage": 703111168
},
{
  "Timestamp": "2021-06-09T22:21:27.51507937Z",
  "host.name": "vm2",
  "service.name": "webserver",
  "state": "buffered",
  "system.memory.usage": 28291072
},
{
  "Timestamp": "2021-06-09T22:21:27.51507937Z",
  "host.name": "vm2",
  "service.name": "webserver",
  "state": "slab_reclaimable",
  "system.memory.usage": 41844736
},
{
  "Timestamp": "2021-06-09T22:21:27.51507937Z",
  "host.name": "vm2",
  "service.name": "webserver",
  "state": "free",
  "system.memory.usage": 79982592
}
```

## How we repackage metrics for wider events

OpenTelemetry Collector has a modular design that allows users to collect metrics from a variety of sources, process them arbitrarily, and then export them to various endpoints. In particular, a metrics processor will accept a stream of arbitrary metrics, do some operation on them, and then output a resulting stream of metrics. Multiple processors can be set to run in a pipeline, each modifying the metrics stream in a different way.

In particular, we suggest a pipeline that transforms metrics in the following three ways:

1. [Preaggregate away unnecessary labels](#preaggregate-away-unnecessary-labels)
2. [Extract static, well-known attribute groups into metric names](#extract-static-well-known-attribute-groups-into-metric-names)
3. [Snap timestamps to the previous second](#snap-timestamps-to-the-previous-second)

Using these processors requires a build of opentelemetry-collector that contains the `metricstransform` processor and the `timestamp` processor. The releases section of this repository contains these builds for standard architectures -- or alternatively, you can build a copy yourself using `opentelemetry-collector-builder`. [More details on how to do this can be found here.](./building.md)

### Preaggregate away unnecessary labels

The hostmetrics receiver provides certain metrics that are broken out into very granular timeseries, where most users would be very happy just looking at a coarser aggregation of this data. For example, the `system.filesystem.usage` metric is separated into individual timeseries based on `device`, `type`, `mode`, `mountpoint`, and `state` labels:

```
data_points:{
  labels:{ key:"device" value:"/dev/vda1" }
  labels:{ key:"type" value:"ext4" }
  labels:{ key:"mode" value:"rw" }
  labels:{ key:"mountpoint" value:"/" }
  labels:{ key:"state", value: "used" }
  time_unix_nano:1623277287513691424
  value:2008989696
}
data_points:{
  labels:{ key:"device" value:"/dev/loop0" }
  labels:{ key:"type" value:"squashfs" }
  labels:{ key:"mode" value:"ro" }
  labels:{ key:"mountpoint" value:"/snap/core18/2066" }
  labels:{ key:"state", value: "used" }
  time_unix_nano:1623277287513691424
  value:58195968
}
```

The average user will not need to know filesystem usage broken out by device — usage for the host as a whole should be sufficient. The same holds for type, mode, and mountpoint (which incidentally are all additional detail about the device attribute in this case). In this example we should aggregate these datapoints into a single datapoint using a SUM aggregation (i.e., adding the values together). We should retain the state label. The result should look like this:

```
data_points:{
  time_unix_nano:1623277287513691424
  value:2067185664
  labels:{ key:"state", value: "used" }
}
```

To accomplish this, we use the [`metricstransform` processor](https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/processor/metricstransformprocessor#readme):

```yaml
processors:
  metricstransform:
    transforms:
      - include: system.filesystem.usage
        action: update
        operations:
          - action: aggregate_labels
            aggregation_type: sum
            label_set: []
```

The hostmetrics receiver comes with a [`metadata.yaml` file](https://raw.githubusercontent.com/open-telemetry/opentelemetry-collector-contrib/141da3a5c4a1bf1570372e2890af383dd833167b/receiver/hostmetricsreceiver/metadata.yaml) that lists all of the metrics it is capable of producing, and all of the tags that those metrics may receive. This repository generates a config like the one in the above example, using this yaml file.

In particular, for any metric produced by this collector, we do this transform on any label that is not identified by an enum field. (As an example, [here is what the enum of filesystem.device looks like](https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/141da3a5c4a1bf1570372e2890af383dd833167b/receiver/hostmetricsreceiver/metadata.yaml#L26-L28).) _Exception: network.state (for the metric system.network.connections) is a well-bounded list of values, so we do not aggregate those away._

### Extract static, well-known attribute groups into metric names

Extract certain labels/attributes into their own metrics. Include the value of the attribute in the new metric’s name. (For example, every data point in a `system.memory.usage` metric with a key of `state` and a value of `buffered` would be moved to a new metric with the name `system.memory.usage.buffered`.)

This heuristic operates only on metrics with given, specific names (for example, `system.memory.usage`). All other metrics will pass through the processor unchanged.

For example, we convert the above metrics stream into one that looks like this instead:

```
metrics:{
  name:"system.memory.usage.used"
  description:"Bytes of memory in use."
  unit:"By"
  int_sum:{
    data_points:{
      time_unix_nano:1623277287515079370
      value:217608192
    }
    aggregation_temporality:AGGREGATION_TEMPORALITY_CUMULATIVE
  }
}
metrics:{
  name:"system.memory.usage.free"
  description:"Bytes of memory in use."
  unit:"By"
  int_sum:{
    data_points:{
      time_unix_nano:1623277287515079370
      value:79982592
    }
    aggregation_temporality:AGGREGATION_TEMPORALITY_CUMULATIVE
  }
}
metrics:{
  name:"system.memory.usage.buffered"
  description:"Bytes of memory in use."
  unit:"By"
  int_sum:{
    data_points:{
      time_unix_nano:1623277287515079370
      value:28291072
    }
    aggregation_temporality:AGGREGATION_TEMPORALITY_CUMULATIVE
  }
}
metrics:{
  name:"system.memory.usage.cached"
  description:"Bytes of memory in use."
  unit:"By"
  int_sum:{
    data_points:{
      time_unix_nano:1623277287515079370
      value:703111168
    }
    aggregation_temporality:AGGREGATION_TEMPORALITY_CUMULATIVE
  }
}
metrics:{
  name:"system.memory.usage.slab_reclaimable"
  description:"Bytes of memory in use."
  unit:"By"
  int_sum:{
    data_points:{
      time_unix_nano:1623277287515079370
      value:41844736
    }
    aggregation_temporality:AGGREGATION_TEMPORALITY_CUMULATIVE
  }
}
metrics:{
  name:"system.memory.usage.slab_unreclaimable"
  description:"Bytes of memory in use."
  unit:"By"
  int_sum:{
    data_points:{
      time_unix_nano:1623277287515079370
      value:41148416
    }
    aggregation_temporality:AGGREGATION_TEMPORALITY_CUMULATIVE
  }
}
```

When they are ingested, these will result in a single event in Honeycomb.

```json
{
  "Timestamp": "2021-06-09T22:21:27.51507937Z",
  "host.name": "vm2",
  "service.name": "webserver",
  "system.memory.usage.used": 217608192,
  "system.memory.usage.free": 79982592,
  "system.memory.usage.buffered": 28291072,
  "system.memory.usage.cached": 703111168,
  "system.memory.usage.slab_reclaimable": 41844736,
  "system.memory.usage.slab_unreclaimable": 41148416
}
```

This type of transform is be supported by the [`metricstransform` processor](https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/processor/metricstransformprocessor#readme). (Note that this config is quite verbose, and there is [a bug with update mode](https://github.com/open-telemetry/opentelemetry-collector-contrib/issues/4194) that requires an additional filtering step afterwards.) For example, here is a config that will accomplish the above example:

```yaml
processors:
  metricstransform:
    transforms:
      - include: system.memory.usage
        experimental_match_labels: {state: "used"}
        action: insert
        new_name: "system.memory.usage.used"
      - include: system.memory.usage
        experimental_match_labels: {state: "free"}
        action: insert
        new_name: "system.memory.usage.free"
     - include: system.memory.usage
        experimental_match_labels: {state: "buffered"}
        action: insert
        new_name: "system.memory.usage.buffered"
       - include: system.memory.usage
        experimental_match_labels: {state: "cached"}
        action: insert
        new_name: "system.memory.usage.cached"
      - include: system.memory.usage
        experimental_match_labels: {state: "slab_reclaimable"}
        action: insert
        new_name: "system.memory.usage.slab_reclaimable"
      - include: system.memory.usage
        experimental_match_labels: {state: "slab_unreclaimable"}
        action: insert
        new_name: "system.memory.usage.slab_unreclaimable"
  filter:
    metrics:
      exclude:
        match_type: strict
        metric_names:
          - system.memory.usage

service:
  pipelines:
    metrics:
      receivers: [hostmetrics]
      processors: [metricstransform, filter]
      exporters: [otlp]
```

The hostmetrics receiver [comes with a `metadata.yaml` file](https://raw.githubusercontent.com/open-telemetry/opentelemetry-collector-contrib/141da3a5c4a1bf1570372e2890af383dd833167b/receiver/hostmetricsreceiver/metadata.yaml) that lists all of the metrics it is capable of producing, and all of the tags that those metrics may receive. This repository generates a config like the one in the above example, using this yaml file.

In particular, for any metric produced by the hostmetrics reciever, we do this transform on any label that is identified by an enum field. (As an example, [here is what the enum of `mem.state` looks like](https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/141da3a5c4a1bf1570372e2890af383dd833167b/receiver/hostmetricsreceiver/metadata.yaml#L21-L24).) If there is more than one label with an enum, we should apply them in a consistent order defined by their order in the yaml file.... for example, `system.paging.operations` with `{direction: page_in, type: minor}` is repackaged into `system.paging.operations.page_in.minor` rather than `system.paging.operations.minor.page_in`.

### Snap timestamps to the previous second

Each subcomponent of the hostmetrics receiver will generate datapoints at slightly variable nanosecond timestamps. For example, on my computer a single request contained 7 distinct timestamps, each less than 100ms apart:

```
1626298669697344000 # generated by system.cpu.load_average.*
1626298669697390000 # generated by system.filesystem.*
1626298669697574000 # generated by system.memory.usage
1626298669697627000 # generated by system.network.[packets|dropped|errors|io]
1626298669711564000 # generated by system.network.connections
1626298669780539000 # generated by system.paging.usage
1626298669780566000 # generated by system.paging.[operations|faults]
```

Granularity to the nanosecond is not particularly helpful for the vast majority of users, and if the timestamps are different when the data arrives at the Honeycomb servers, all of these metrics will end up getting saved to different events, reducing the repackaging ratio by a factor of ~7.

We address this by modifying timestamps for all metrics received in a batch by rounding them down to the most recent second. In this example, all timestamps would end up getting set to `1626298669000000000`. If, for some reason, the timestamps in a batch varied by more than 1 second, we would still retain that variability at up to a 1-second granularity.

This transformation can be accomplished by using the `timestampprocessor`, which [currently lives in this repository](../timestampprocessor) but we're working on getting it merged into the official [opentelemetry-collector-contrib](https://github.com/open-telemetry/opentelemetry-collector-contrib) repository instead. It can be configured like this:

```yaml
processors:
  timestamp:
    round_to_nearest: 1s

service:
  pipelines:
    metrics:
      receivers: [hostmetrics]
      processors: [timestamp]
      exporters: [otlp]
```
